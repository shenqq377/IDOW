<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Solving Instance Detection from an Open-World Perspective." />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/insdet_idow.jpg" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Solving Instance Detection from an Open-World Perspective.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/insdet_idow.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Instance Detection, Open-World Learning, Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Solving Instance Detection from an Open-World Perspective.</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Solving Instance Detection from an Open-World Perspective
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://shenqq377.github.io/" target="_blank">Qianqian Shen</a><sup>1</sup></span>
              <span class="author-block">
                <a href="https://yunhan-zhao.github.io/" target="_blank">Yunhan Zhao</a><sup>2</sup></span>
              <span class="author-block">
                <a href="https://nahyunkwon.github.io/" target="_blank">Nahyun Kwon</a><sup>3</sup></span>
              <span class="author-block">
                <a href="https://github.com/qubick" target="_blank">Jeeeun Kim</a><sup>3</sup></span>
              <span class="author-block">
                <a href="https://yananlix1.github.io/" target="_blank">Yanan Li</a><sup>4</sup></span>
              <span class="author-block">
                <a href="https://aimerykong.github.io/" target="_blank">Shu Kong</a><sup>3</sup><sup>,</sup><sup>5</sup><sup>,</sup><sup>6</sup></span>
                
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University<br></span>
              <span class="author-block"><sup>2</sup>UC Irvine<br></span>     
              <span class="author-block"><sup>3</sup>Texas A&M University<br></span>  
              <span class="author-block"><sup>4</sup>Zhejiang Lab<br></span>                
              <span class="author-block"><sup>5</sup>University of Macau<br></span>                
              <span class="author-block"><sup>6</sup>Institute of Collaborative
                Innovation<br></span>  
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.00359" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/shenqq377/IDOW" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.00359" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
            <h1><strong style="color: red; font-size: x-large;">CVPR 2025</strong></h1>
            <!-- <h2 class="subtitle has-text-centered">
              <b>tl;dr:</b> We adapt a pretrained Vision-Language Model and repurpose its<br>pretraining data to boost few-shot recognition performance</h2> -->

          </div>
        </div>
      </div>  
    </div>
  </section>

    <!-- Paper overview -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                Instance detection (InsDet) aims to localize specific object instances within 
                a novel scene imagery based on given visual references. We elaborate the 
                open-world challenge of InsDet: (1) the testing data distribution is unknown 
                during training, and (2) there are domain gaps between visual references and 
                detected proposals.
              </p>    
              <p style="text-align: justify;">              
                Movitated by the InsDet's <b>open-world nature</b>, we exploit diverse 
                <b>open</b> data and foundation models to solve InsDet in the open world. 
                To better adapt FM for instance-level feature matching, we introduce 
                <b>distractor sampling</b> to sample patches of random background images as 
                universal negative data to all object instances, and <b>novel-view synthesis</b> 
                generate more visual references not only training but for testing. Our IDOW 
                outperforms prior works by >10 AP in both conventional and novel instance 
                detection settings.
              </p>         
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper overview -->

  <!-- Paper abstract -->
  <!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">
              Instance detection (InsDet) aims to localize specific object instances 
              within a novel scene imagery based on given visual references. Technically, 
              it requires proposal detection to identify all possible object instances, 
              followed by instance-level matching to pinpoint the ones of interest. Its 
              open-world nature supports its broad applications from robotics to AR/VR 
              but also presents significant challenges: methods must generalize to unknown 
              testing data distributions because (1) the testing scene imagery is unseen 
              during training, and (2) there are domain gaps between visual references and 
              detected proposals. Existing methods tackle these challenges by synthesizing 
              diverse training examples or utilizing off-the-shelf foundation models (FMs). 
              However, they only partially capitalize the available open-world information. 
              In tcontrast, we approach InsDet from an Open-World perspective, introducing 
              our method IDOW. We find that, while pretrained FMs yield high recall in 
              instance detection, they are not specifically optimized for instance-level 
              feature matching. Therefore, we adapt pretrained FMs for improved instance-level 
              matching using open-world data. Our approach incorporates metric learning along 
              with novel data augmentations, which sample distractors as negative examples and 
              synthesize novel-view instances to enrich the visual references. Extensive 
              experiments demonstrate that our method significantly outperforms prior works, 
              achieving $>$10 AP over previous results on two recently released challenging 
              benchmark datasets in both conventional and novel instance detection settings.
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End paper abstract -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Findings</h2>          
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <img src="static/images/insdet_findings.jpg" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Existing InsDet methods leverage the open-world information in different aspects: (a) <b>background image sampling</b> 
                (from the open world) to synthesize training data, (b) <b>object image sampling</b> (from the open world) to learn feature 
                representations,and (c) <b>foundation model utilization</b> (pretrained in the open world) for proposal detection and 
                instance-level feature matching. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Solution</h2>          
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <img src="static/images/insdet_idow.jpg" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                As FMs are not specifcally designed for instance-level feature matching required by InsDet, we propose to adapt them
                by leveraging rich datasampled from the open world. We gather data from multiple sources: 
                <ol>
                  <li>Any available visual references of instances in the CID setting;</li>
                  <li>Abundant multi-view object images sampled in the open world similar to object image sampling;</li>
                  <li>Synthetic data by training NeRF to generate novel-view images based on the given instances;</li>
                  <li>Distractors by running FMs (esp. SAM) on random open-world imagery to generate random object-like proposals.</li>
                </ol>
                We use the data above to adapt FM through metric learning. The technical novelty of our work lies in the last two 
                sources, as well as the design choice of metric learning to adapt FMs for InsDet.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method Overview -->

  
  <!-- Benchmark -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <h2 class="title is-4">IDOW achieves state-of-the-art performances</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/Benchmarking_CID.png" alt="1" style="width: 800px; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                Remarkably, we show that
                <ul>
                  <li>IDOW signiffcantly outperforms previous methods, e.g., IDOW<sub>GroundingDINO</sub> (57.01 AP) > OTS-FM<sub>GroundingDINO</sub> (51.68 AP) > CPL<sub>DINO</sub> (27.99 AP). 
                      This conffrms the importance of addressing InsDet from the open-world perspective. 
                  <li>Second, adapting FMs by our IDOW further boosts the performance by 5-7 AP, e.g., IDOW<sub>SAM</sub> (48.75 AP) > OTS-FM<sub>SAM</sub> (41.61 AP). </li>
                  <li>IDOW and OTS-FM are applicable to different pretrained FMs and adopting stronger FMs achieves better performance, 
                      e.g., using GroundingDINO yields >8 AP than SAM in IDOW.</li>
                </ul>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Benchmark -->


  <!-- Ablation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
<!--           <h2 class="title is-3">Results</h2> -->
          <h2 class="title is-4">IDOW adapt FMs with diverse open data sources</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/ablation.png" alt="1" style="width: 800px; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We use OTS-FMGroundingDINO as a baseline over which we incrementally add each strategy. 
                <b>Train</b> means foundation model adaptation through fnetuning on the available data. 
                <b>DA</b> denotes Data Augmentation with NeRF-generated novel-views; 
                <b>DS</b> denotes Distractor Sampling. Results clearly demonstrate that all the four 
                strategies help achieve better InsDet performance. 
                <ul>
                  <li>Finetuning FMs on the given visual references enhances detection performance, cf. Train (53.94 AP) > baseline (51.68 AP).</li>
                  <li>Moreover, NeRF-based data augmentation improves the final detection performance, particularly when used in testing, 
                  cf. Train+DA@Test (56.44 AP) > Train+DA@Train (54.48 AP) > baseline (51.68AP).</li>
                  <li>Lastly, applying distractor sampling (DS) improves the ffnal performance further, cf. Train+DS (54.10 AP) > Train (53.94 AP).</li>
                </ul>  
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Ablation -->


  <!-- Ablation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
<!--           <h2 class="title is-3">Results</h2> -->
          <h2 class="title is-4">Comparison by finetuning FM and other backbones</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/t-SNE.png" alt="1" style="width: 800px; height: auto; display: block; margin: 0 auto;"/> 
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                We compare our finetuned DINOv2 against a finetuned ImageNet-pretrained ResNet101 model and the baseline 
                instance detector CPL with a FasterRCNN architecture. Visually, the finetuned DINOv2 extracts more 
                discriminative features.  
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Ablation -->

  <!-- Visualization -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
<!--           <h2 class="title is-3">Results</h2> -->
          <h2 class="title is-4">More visualization</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/InsDet_hard.png" alt="1" style="width: 800px; height: auto; display: block; margin: 0 auto;"/> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Visualization -->
  
  <!-- Acknowledgement -->


  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">BibTeX</h2>
      <p> If you find our work useful, please consider citing our papers:</p>
      <pre><code>@inproceedings{shen2025solving,
        title={Solving Instance Detection from an Open-World Perspective},
        author={Shen, Qianqian and Zhao, Yunhan and Kwon, Nahyun and Kim, Jeeeun and Li, Yanan and Kong, Shu},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        year={2025}
      }</code></pre>
      <pre><code>@inproceedings{shen2023high,
        title={A high-resolution dataset for instance detection with multi-view object capture},
        author={Shen, Qianqian and Zhao, Yunhan and Kwon, Nahyun and Kim, Jeeeun and Li, Yanan and Kong, Shu},
        booktitle={NeurIPS Datasets & Benchmark Track},
        year={2023}
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
